{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import dataset",
   "id": "9103784e96014212"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-26T15:21:15.401971Z",
     "start_time": "2025-09-26T15:21:00.156254Z"
    }
   },
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fetch dataset\n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = default_of_credit_card_clients.data.features\n",
    "y = default_of_credit_card_clients.data.targets\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Start preprocessing of data",
   "id": "9ae19bdae9e610ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:21:34.664377Z",
     "start_time": "2025-09-26T15:21:34.642306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# combine X and Y to see how many rows have NaN\n",
    "combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# drop rows with NaN\n",
    "combined = combined.dropna(how='any')\n",
    "\n",
    "# split back into X and y\n",
    "X = combined.drop(columns=['Y'])  # drop the 'Y' column\n",
    "y = combined.drop(columns=X.columns)\n"
   ],
   "id": "44c36e966cfec1f2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split and scale data",
   "id": "fb9fd6aab2bfed97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:24:08.920784Z",
     "start_time": "2025-09-26T15:24:08.878713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# use our scaler to fit and transform our training data\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_test.columns)\n",
    "\n",
    "# todo: why do we have to scale this too!\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# add our intercepts\n",
    "X.insert(0, \"intercept\", 1)        # after scaling they are separate dataframes\n",
    "X_test.insert(0, \"intercept\", 1)\n",
    "X_train.insert(0, \"intercept\", 1)"
   ],
   "id": "52ffc08612009b8c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize our Hyperparameters for Gradient Descent",
   "id": "a959141875ecbf2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:24:13.278568Z",
     "start_time": "2025-09-26T15:24:13.271538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize B to 0\n",
    "beta_gd = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Set learning rate eta\n",
    "eta = 0.000001\n",
    "\n",
    "# Set number of iterations\n",
    "max_iter = 20000\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 20\n",
    "\n",
    "# Set epochs (for SGD)\n",
    "epochs = 5"
   ],
   "id": "a5e1b67c3b7e322f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#TODO: Below is very subject to change",
   "id": "2379eb2cbf3b3728"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:24:15.161880Z",
     "start_time": "2025-09-26T15:24:15.156929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gradient function that can take different sizes (even 1 point)\n",
    "def gradient(x_train_batch, y_train_batch, beta):\n",
    "    return -2 * x_train_batch.T @ y_train_batch + 2 * x_train_batch.T @ (x_train_batch @ beta)"
   ],
   "id": "4862e183947223ff",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:25:55.189268Z",
     "start_time": "2025-09-26T15:24:16.193658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "# Train model\n",
    "for _ in range(max_iter):\n",
    "    # not shuffling data for simplicity\n",
    "    grad = gradient(X_train.values, y_train.values, beta_gd)\n",
    "    beta_gd = beta_gd - eta * grad\n",
    "\n",
    "# Predict on test set\n",
    "y_test_hat = X_test.values.dot(beta_gd)\n",
    "residuals = y_test.values - y_test_hat\n",
    "\n",
    "# Calculate RSS\n",
    "RSS = (residuals.T.dot(residuals))\n",
    "print(RSS)"
   ],
   "id": "b0a75af4eb91c1f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[887.34466549]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T23:52:12.789861Z",
     "start_time": "2025-09-25T23:52:12.725972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stochastic Gradient Descent\n",
    "for _ in range(epochs):\n",
    "    # Shuffle data\n",
    "    new_indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuf = X_train.values[new_indices]\n",
    "    y_train_shuf = y_train.values[new_indices]\n",
    "\n",
    "    for i in range(len(X_train_shuf)):\n",
    "        grad = gradient(np.atleast_2d(X_train_shuf[i]), y_train_shuf, beta_gd)\n",
    "        beta_gd = beta_gd - eta * grad\n",
    "\n",
    "y_test_hat = X_test.values.dot(beta_gd)\n",
    "residuals = y_test.values - y_test_hat\n",
    "\n",
    "# Calculate RSS\n",
    "RSS = (residuals.T.dot(residuals))\n",
    "print(RSS)"
   ],
   "id": "1071b7bf24a24e1f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      6\u001B[39m     y_train_shuf = y_train.values[new_indices]\n\u001B[32m      8\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X_train_shuf)):\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m         grad = \u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43matleast_2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_shuf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_shuf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta_gd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m         beta_gd = beta_gd - eta * grad\n\u001B[32m     12\u001B[39m y_test_hat = X_test.values.dot(beta_gd)\n",
      "\u001B[31mTypeError\u001B[39m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mini-Batch Gradient Descent\n",
    "for _ in range(epochs):\n",
    "    # Shuffle data\n",
    "    new_indices = np.random.permutation(len(X_train))\n",
    "    X_train_shuf = X_train.values[new_indices]\n",
    "    y_train_shuf = y_train.values[new_indices]\n",
    "\n",
    "    # todo: more qualifications on randomizing batch?\n",
    "    for i in range(len(X_train_shuf)):\n",
    "        gradient = gradient(np.atleast_2d(X_train_shuf[i:i+batch_size]), np.atleast_2d(y_train_shuf[i:i+batch_size]), beta_gd)\n",
    "        beta_gd = beta_gd - eta * gradient\n",
    "\n",
    "y_test_hat = X_test.values.dot(beta_gd)\n",
    "residuals = y_test.values - y_test_hat\n",
    "\n",
    "# Calculate RSS\n",
    "RSS = (residuals.T.dot(residuals))\n",
    "print(RSS)"
   ],
   "id": "3198b8b9b59f4e91",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
